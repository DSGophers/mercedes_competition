{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (4209, 378)\n",
      "Shape test: (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# read datasets\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "train_idx = train.ID.tolist()\n",
    "test_idx = test.ID.tolist()\n",
    "\n",
    "y_train = train[\"y\"]\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "full_df = pd.concat([train.drop('y', axis=1), test])\n",
    "full_df = full_df.set_index('ID')\n",
    "\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in full_df.columns:\n",
    "    if full_df[c].dtype == 'object':\n",
    "        full_df = pd.concat([full_df, pd.get_dummies(full_df[c], prefix=c)], axis=1)\n",
    "        full_df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))\n",
    "\n",
    "##Add decomposed components: PCA / ICA etc.\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "tsvd_results_full = tsvd.fit_transform(full_df)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_full = pca.fit_transform(full_df)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42, max_iter=1000, tol=0.01)\n",
    "ica2_results_full = ica.fit_transform(full_df)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    full_df['pca_' + str(i)] = pca2_results_full[:, i-1]\n",
    "    full_df['ica_' + str(i)] = ica2_results_full[:, i-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.382\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "import pylab as pl\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.cluster import DBSCAN\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#cluster3d(full_df, {'n_components': 5, 'eps': 0.500000, 'min_samples': 22, 'n_jobs': -1}, 'euclidean')\n",
    "best_params = {'n_components': 11, 'eps': 0.750000, 'min_samples': 2, 'n_jobs': -1}\n",
    "\n",
    "pca = decomposition.PCA(n_components=best_params['n_components'])\n",
    "full_df_reduced = pca.fit_transform(full_df)\n",
    "full_df_reduced = StandardScaler().fit_transform(full_df_reduced)\n",
    "full_df_reduced = pd.DataFrame(full_df_reduced, index = full_df.index.values)\n",
    "\n",
    "clusters = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'], n_jobs=best_params['n_jobs']).fit_predict(full_df_reduced)\n",
    "clusters_labels = pd.unique(pd.Series(clusters))\n",
    "clusters_df = pd.DataFrame(clusters, index = full_df_reduced.index, columns=['cluster'])\n",
    "clusters_labels_all = clusters_df\n",
    "count = pd.Series(clusters).value_counts()\n",
    "score = metrics.silhouette_score(full_df_reduced, clusters)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([full_df, clusters_df], axis=1)\n",
    "\n",
    "full_df = pd.concat([full_df, pd.get_dummies(full_df.cluster, prefix='cluster')], axis=1)\n",
    "full_df.drop('cluster', axis=1, inplace=True)\n",
    "\n",
    "X_train = full_df.loc[train_idx]\n",
    "X_test = full_df.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565246623513\n"
     ]
    }
   ],
   "source": [
    "##### Regressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = {\n",
    "    'n_trees': 1500,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'subsample': 0.65,\n",
    "    'objective': 'reg:linear',\n",
    "    #'objective': 'reg:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 0,\n",
    "    'n_jobs': -1,\n",
    "    #'tree_method': 'hist',\n",
    "    #'max_bin': 255,\n",
    "    'booster': 'gbtree',\n",
    "    'rate_drop': 0.10,\n",
    "    #'alpha': 0.05\n",
    "}\n",
    "\"\"\"\n",
    "    rate_drop [default=0.0]\n",
    "        dropout rate (a fraction of previous trees to drop during the dropout).\n",
    "        range: [0.0, 1.0]\n",
    "    booster [default=gbtree]\n",
    "        which booster to use, can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.\n",
    "    lambda [default=1, alias: reg_lambda]\n",
    "        L2 regularization term on weights, increase this value will make model more conservative.\n",
    "    alpha [default=0, alias: reg_alpha]\n",
    "        L1 regularization term on weights, increase this value will make model more conservative.\n",
    "\"\"\"\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "num_boost_rounds = 2500\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "# check r2-score (to get higher score - increase num_boost_round in previous cell)\n",
    "print(r2_score(model.predict(dtrain), dtrain.get_label()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropout + L1 = 0.565941022665 valid 0.38 test\n",
    "#only dropout = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.472392002894\n",
    "\n",
    "xgb.train(data = xgb_data,\n",
    "                        nthread = i,\n",
    "                        nrounds = 200,\n",
    "                        max_leaves = 255,\n",
    "                        max_depth = 12,\n",
    "                        eta = 0.05,\n",
    "                        tree_method = \"hist\",\n",
    "                        max_bin = 255,\n",
    "                        booster = \"gbtree\",\n",
    "                        objective = \"binary:logistic\",\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(model.predict(dtest), 'xgboost_clustering_hist_dropout.csv', index=X_test.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# monitor training performance\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "eval_set = [(X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_metric=\"error\", eval_set=eval_set, verbose=True)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "\n",
    "def score(params):\n",
    "    print \"Training with params : \"\n",
    "    print params\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    # watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "    predictions = model.predict(dvalid).reshape((X_test.shape[0], 9))\n",
    "    score = log_loss(y_test, predictions)\n",
    "    print \"\\tScore {0}\\n\\n\".format(score)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 1000, 1),\n",
    "             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "             'max_depth' : hp.quniform('max_depth', 1, 13, 1),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "             'num_class' : 9,\n",
    "             'eval_metric': 'mlogloss',\n",
    "             'objective': 'multi:softprob',\n",
    "             'nthread' : 6,\n",
    "             'silent' : 1\n",
    "             }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=250)\n",
    "\n",
    "    print best\n",
    "\n",
    "\n",
    "X, y = load_train()\n",
    "print \"Splitting data into train and valid ...\\n\\n\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted, out_file, target='y', index=None, index_label=\"ID\"):\n",
    "    if index is None:\n",
    "        index = np.arange(1, predicted.shape[0] + 1)\n",
    "    predicted_df = pd.DataFrame(predicted,\n",
    "                                index = index,\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
